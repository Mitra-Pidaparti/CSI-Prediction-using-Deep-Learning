{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPs9OBS594LPCMxao0Fq2bz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XxZ1EHu1a1d0"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","# Define the loss function\n","criterion = nn.MSELoss()\n","\n","# Define the compression ratios\n","compression_ratios = [4,8]\n","\n","def calculate_mse(outputs, targets):\n","    mse = criterion(outputs, targets)\n","    return mse\n","\n","def NMSE(outputs, inputs_resized):\n","    outputs_real = torch.reshape(outputs[:, 0, :, :], (outputs.size(0), -1))\n","    outputs_imag = torch.reshape(outputs[:, 1, :, :], (outputs.size(0), -1))\n","    outputs_comp = (outputs_real - 0.5) + 1j * (outputs_imag - 0.5)\n","\n","    inputs_resized_real = torch.reshape(inputs_resized[:, 0, :, :], (inputs_resized.size(0), -1))\n","    inputs_resized_imag = torch.reshape(inputs_resized[:, 1, :, :], (inputs_resized.size(0), -1))\n","    inputs_resized_comp = (inputs_resized_real - 0.5) + 1j * (inputs_resized_imag - 0.5)\n","\n","    mse = torch.mean(torch.abs(outputs_comp - inputs_resized_comp) ** 2, dim=1)\n","    power = torch.mean(torch.abs(inputs_resized_comp) ** 2, dim=1)\n","\n","    nmse = 10 * torch.log10(torch.mean(mse / power))\n","\n","    return nmse\n","\n","\n","import torch.nn as nn\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, compressed_dim, height, width):\n","        super(EncoderBlock, self).__init__()\n","        self.conv = nn.Conv2d(2, 2, kernel_size=3, stride=1, padding=1)  # Adjusted number of input channels to 4\n","        self.bn = nn.BatchNorm2d(2)\n","        self.relu = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","        self.dense = nn.Linear(2 * height * width, compressed_dim)\n","        self.height = height\n","        self.width = width\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return x\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, compressed_dim, height, width):\n","        super(DecoderBlock, self).__init__()\n","        self.compressed_dim = compressed_dim\n","        self.height = height\n","        self.width = width\n","        self.dense = nn.Linear(compressed_dim, 2 * (height) * (width))\n","        self.conv1 = nn.Conv2d(2, 4, kernel_size=1, stride=1)\n","        self.bn1 = nn.BatchNorm2d(4)\n","        self.relu1 = nn.ReLU()\n","        self.conv2_upper = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n","        self.bn2_upper = nn.BatchNorm2d(4)\n","        self.conv2_upper2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n","        self.bn2_upper2 = nn.BatchNorm2d(8)\n","        self.conv2_lower = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n","        self.bn2_lower = nn.BatchNorm2d(4)\n","        self.conv2_lower2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n","        self.bn2_lower2 = nn.BatchNorm2d(8)\n","        self.conv3 = nn.Conv2d(16, 4, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(4)\n","        self.conv4 = nn.Conv2d(4, 2, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(2)\n","        self.reconstruction = nn.Conv2d(2, 2, kernel_size=3, stride=1, padding=1)\n","        self.sigmoid=nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = x.view(-1, 2, self.height, self.width)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","\n","        upper_branch = self.conv2_upper(x)\n","        upper_branch = self.bn2_upper(upper_branch)\n","\n","        upper_branch = self.conv2_upper2(upper_branch)\n","        upper_branch = self.bn2_upper2(upper_branch)\n","\n","        lower_branch = self.conv2_lower(x)\n","        lower_branch = self.bn2_lower(lower_branch)\n","\n","        lower_branch = self.conv2_lower2(lower_branch)\n","        lower_branch = self.bn2_lower2(lower_branch)\n","\n","        x = torch.cat([upper_branch, lower_branch], dim=1)\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        x = self.reconstruction(x)\n","\n","        x_reshaped = x.view(-1, 2, self.height, self.width)\n","        x = x + x_reshaped\n","        x = self.sigmoid(x)\n","        return x\n","\n","\n","class LightweightCNN(nn.Module):\n","    def __init__(self, compressed_dim, height, width):\n","        super(LightweightCNN, self).__init__()\n","        self.encoder = EncoderBlock(compressed_dim, height, width)\n","        self.decoder = DecoderBlock(compressed_dim, height, width)\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","\n","# Define the folder path containing the dataset files\n","data_folder_path = r\"C:\\Users\\mitra\\OneDrive\\Desktop\\Model4 dataset\\Model4 train dataset\"\n","# Get the list of dataset file names in the folder\n","file_names = sorted(os.listdir(data_folder_path))\n","\n","# Load the data from all dataset files and combine them\n","combined_data = []\n","for file_name in file_names:\n","    file_path = os.path.join(data_folder_path, file_name)\n","    data = torch.from_numpy(sio.loadmat(file_path)[\"x\"])\n","    combined_data.append(data)\n","\n","combined_data = torch.cat(combined_data, dim=0)\n","\n","# Shuffle the combined dataset\n","shuffled_indices = torch.randperm(len(combined_data))\n","combined_data_shuffled = combined_data[shuffled_indices]\n","\n","# Create a DataLoader with the shuffled combined dataset\n","batch_size = 100\n","shuffled_loader = DataLoader(combined_data_shuffled.float(), batch_size=batch_size, shuffle=True)\n","\n","# Split the shuffled combined data into train and validation sets\n","train_data, val_data = train_test_split(combined_data_shuffled, test_size=0.2, random_state=42)\n","\n","# Normalize the datasets\n","max_abs_train = torch.max(torch.abs(train_data))\n","max_abs_val = torch.max(torch.abs(val_data))\n","\n","train_data_normalized = train_data / max_abs_train * 0.5 + 0.5\n","val_data_normalized = val_data / max_abs_val * 0.5 + 0.5\n","# Create a new folder to save model parameters\n","save_folder = r\"C:\\Users\\mitra\\OneDrive\\Desktop\\Lightweight model parameters for different CRs\\All_freq_subband\"\n","\n","for cr in compression_ratios:\n","    print(f\"Training and validating for compression ratio 1/{cr}\")\n","\n","    def plot_train_loss(train_loss_list):\n","        train_loss = train_loss_list\n","        plt.plot(range(1, num_epochs + 1), train_loss)\n","        plt.xlabel('Epoch')\n","        plt.ylabel('(Training Loss)')\n","        plt.title('Training Loss vs. Epoch')\n","        plt.show()\n","\n","    train_loss_list = []\n","\n","    # Define the dimensions\n","    compressed_dim = int(2 * 32 * 32 * (1 / cr))\n","    height = 32\n","    width = 32\n","    model = LightweightCNN(compressed_dim, height, width)\n","    # Define the optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Create DataLoaders for training and validation data\n","    train_loader = DataLoader(train_data_normalized.float(), batch_size=100, shuffle=True)\n","    val_loader = DataLoader(val_data_normalized.float(), batch_size=100,shuffle=True)\n","\n","    # Train the model\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.train()\n","\n","    num_epochs = 200  # Increase the number of epochs for better training\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        for inputs in train_loader:\n","            inputs = inputs.to(device)\n","\n","            # Zero the gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Resize the input tensor to match the output shape\n","            inputs_resized = inputs[:, :2, :, :]\n","\n","            # Compute the MSE loss\n","            loss = criterion(outputs, inputs_resized)\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Update the running loss\n","            running_loss += loss.item() * inputs.size(0)\n","\n","        # Calculate the average loss for the training set\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_loss_list.append(train_loss)\n","\n","        # Validate the model\n","        model.eval()\n","        running_loss = 0.0\n","\n","        with torch.no_grad():\n","            for val_inputs in val_loader:\n","                val_inputs = val_inputs.to(device)\n","\n","                # Forward pass\n","                val_outputs = model(val_inputs)\n","\n","                # Resize the input tensor to match the output shape\n","                val_inputs_resized = val_inputs[:, :2, :, :]\n","\n","                # Compute the MSE loss\n","                val_loss = criterion(val_outputs, val_inputs_resized)\n","\n","                # Update the running loss\n","                running_loss += val_loss.item() * val_inputs.size(0)\n","\n","        # Calculate the average loss for the validation set\n","        val_loss = running_loss / len(val_loader.dataset)\n","\n","        # Print the epoch and loss for training and validation sets\n","        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss} - Val Loss: {val_loss}\")\n","    plot_train_loss(train_loss_list)\n","\n","    # Save the model/weights\n","    save_path = os.path.join(save_folder, f\"CR_{cr}\")\n","    os.makedirs(save_path, exist_ok=True)\n","    torch.save(model.state_dict(), os.path.join(save_path, \"model_weights.pth\"))\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import torch.nn.functional as F\n","\n","# Define the loss function\n","criterion = nn.MSELoss()\n","\n","# Define the compression ratios\n","compression_ratios = [8, 16, 32, 64]\n","\n","def calculate_mse(outputs, targets):\n","    mse = criterion(outputs, targets)\n","    return mse\n","\n","def NMSE(outputs, inputs_resized):\n","    outputs_real = torch.reshape(outputs[:, 0, :, :], (outputs.size(0), -1))\n","    outputs_imag = torch.reshape(outputs[:, 1, :, :], (outputs.size(0), -1))\n","    outputs_comp = (outputs_real - 0.5) + 1j * (outputs_imag - 0.5)\n","\n","    inputs_resized_real = torch.reshape(inputs_resized[:, 0, :, :], (inputs_resized.size(0), -1))\n","    inputs_resized_imag = torch.reshape(inputs_resized[:, 1, :, :], (inputs_resized.size(0), -1))\n","    inputs_resized_comp = (inputs_resized_real - 0.5) + 1j * (inputs_resized_imag - 0.5)\n","\n","    mse = torch.mean(torch.abs(outputs_comp - inputs_resized_comp) ** 2, dim=1)\n","    power = torch.mean(torch.abs(inputs_resized_comp) ** 2, dim=1)\n","\n","    nmse = 10 * torch.log10(torch.mean(mse / power))\n","\n","    return nmse\n","\n","\n","import torch.nn as nn\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, compressed_dim, height, width):\n","        super(EncoderBlock, self).__init__()\n","        self.conv = nn.Conv2d(2, 2, kernel_size=3, stride=1, padding=1)  # Adjusted number of input channels to 4\n","        self.bn = nn.BatchNorm2d(2)\n","        self.relu = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","        self.dense = nn.Linear(2 * height * width, compressed_dim)\n","        self.height = height\n","        self.width = width\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return x\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, compressed_dim, height, width):\n","        super(DecoderBlock, self).__init__()\n","        self.compressed_dim = compressed_dim\n","        self.height = height\n","        self.width = width\n","        self.dense = nn.Linear(compressed_dim, 2 * (height) * (width))\n","        self.conv1 = nn.Conv2d(2, 4, kernel_size=1, stride=1)\n","        self.bn1 = nn.BatchNorm2d(4)\n","        self.relu1 = nn.ReLU()\n","        self.conv2_upper = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n","        self.bn2_upper = nn.BatchNorm2d(4)\n","        self.conv2_upper2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n","        self.bn2_upper2 = nn.BatchNorm2d(8)\n","        self.conv2_lower = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n","        self.bn2_lower = nn.BatchNorm2d(4)\n","        self.conv2_lower2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n","        self.bn2_lower2 = nn.BatchNorm2d(8)\n","        self.conv3 = nn.Conv2d(16, 4, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(4)\n","        self.conv4 = nn.Conv2d(4, 2, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(2)\n","        self.reconstruction = nn.Conv2d(2, 2, kernel_size=3, stride=1, padding=1)\n","        self.sigmoid=nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = x.view(-1, 2, self.height, self.width)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","\n","        upper_branch = self.conv2_upper(x)\n","        upper_branch = self.bn2_upper(upper_branch)\n","\n","        upper_branch = self.conv2_upper2(upper_branch)\n","        upper_branch = self.bn2_upper2(upper_branch)\n","\n","        lower_branch = self.conv2_lower(x)\n","        lower_branch = self.bn2_lower(lower_branch)\n","\n","        lower_branch = self.conv2_lower2(lower_branch)\n","        lower_branch = self.bn2_lower2(lower_branch)\n","\n","        x = torch.cat([upper_branch, lower_branch], dim=1)\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        x = self.reconstruction(x)\n","\n","        x_reshaped = x.view(-1, 2, self.height, self.width)\n","        x = x + x_reshaped\n","        x = self.sigmoid(x)\n","        return x\n","\n","\n","class LightweightCNN(nn.Module):\n","    def __init__(self, compressed_dim, height, width):\n","        super(LightweightCNN, self).__init__()\n","        self.encoder = EncoderBlock(compressed_dim, height, width)\n","        self.decoder = DecoderBlock(compressed_dim, height, width)\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","\n","height = 32\n","width = 32\n","# Define the folder path containing the test dataset files\n","test_folder_path = r\"C:\\Users\\mitra\\OneDrive\\Desktop\\Model4 dataset\\Model4 test dataset\"\n","\n","# Get the list of test dataset file names in the folder\n","test_file_names = sorted(os.listdir(test_folder_path))\n","\n","# Load the test data from all test dataset files and combine them\n","test_data = []\n","for test_file_name in test_file_names:\n","    test_file_path = os.path.join(test_folder_path, test_file_name)\n","    test_data.append(torch.from_numpy(sio.loadmat(test_file_path)[\"x\"]))\n","\n","test_data = torch.cat(test_data, dim=0)\n","\n","# Normalize the test data\n","max_abs_test = torch.max(torch.abs(test_data))\n","test_data_normalized = test_data / max_abs_test * 0.5 + 0.5\n","\n","# Define the function to calculate NMSE\n","def NMSE(outputs, inputs_resized):\n","    outputs_real = torch.reshape(outputs[:, 0, :, :], (outputs.size(0), -1))\n","    outputs_imag = torch.reshape(outputs[:, 1, :, :], (outputs.size(0), -1))\n","    outputs_comp = (outputs_real - 0.5) + 1j * (outputs_imag - 0.5)\n","\n","    inputs_resized_real = torch.reshape(inputs_resized[:, 0, :, :], (inputs_resized.size(0), -1))\n","    inputs_resized_imag = torch.reshape(inputs_resized[:, 1, :, :], (inputs_resized.size(0), -1))\n","    inputs_resized_comp = (inputs_resized_real - 0.5) + 1j * (inputs_resized_imag - 0.5)\n","\n","    mse = torch.mean(torch.abs(outputs_comp - inputs_resized_comp) ** 2, dim=1)\n","    power = torch.mean(torch.abs(inputs_resized_comp) ** 2, dim=1)\n","    nmse = 10 * torch.log10(torch.mean(mse / power))\n","    return nmse\n","\n","# Define the folder paths containing the saved model weights\n","model_weights_folder = r\"C:\\Users\\mitra\\OneDrive\\Desktop\\Lightweight model parameters for different CRs\\All_freq_subband\"\n","\n","model_weights_1_4 = os.path.join(model_weights_folder, \"CR_4\", \"model_weights.pth\")\n","model_weights_1_8 = os.path.join(model_weights_folder, \"CR_8\", \"model_weights.pth\")\n","\n","# Create DataLoader for test data\n","test_loader = DataLoader(test_data_normalized.float(), batch_size=1, shuffle=False)\n","\n","# Test the model for each file in the dataset\n","for i, inputs in enumerate(test_loader, 1):\n","    inputs = inputs.to(device)\n","\n","    # Resize the input tensor to match the output shape\n","    inputs_resized = inputs[:, :2, :, :]\n","\n","    # Print the file name\n","    file_name = test_file_names[i - 1]\n","    print(f\"File: {file_name}\")\n","\n","    # Test the model for compression ratio 1/4\n","    print(\"Testing for compression ratio 1/4\")\n","    compressed_dim = int(2 * 32 * 32 * (1 / 4))\n","    model_1_4 = LightweightCNN(compressed_dim, height, width)\n","    model_1_4.load_state_dict(torch.load(model_weights_1_4))\n","    model_1_4.to(device)\n","    model_1_4.eval()\n","\n","    # Forward pass\n","    outputs_1_4 = model_1_4(inputs)\n","\n","    # Compute the NMSE and MSE losses for compression ratio 1/4\n","    loss_1_4 = NMSE(outputs_1_4, inputs_resized)\n","    loss_mse_1_4 = F.mse_loss(outputs_1_4, inputs_resized)\n","\n","    print(f\"Compression Ratio 1/4 - NMSE: {loss_1_4.item()} - MSE: {loss_mse_1_4.item()}\")\n","\n","    # Test the model for compression ratio 1/8\n","    print(\"Testing for compression ratio 1/8\")\n","    compressed_dim = int(2 * 32 * 32 * (1 / 8))\n","    model_1_8 = LightweightCNN(compressed_dim, height, width)\n","    model_1_8.load_state_dict(torch.load(model_weights_1_8))\n","    model_1_8.to(device)\n","    model_1_8.eval()\n","\n","    # Forward pass\n","    outputs_1_8 = model_1_8(inputs)\n","\n","    # Compute the NMSE and MSE losses for compression ratio 1/8\n","    loss_1_8 = NMSE(outputs_1_8, inputs_resized)\n","    loss_mse_1_8 = F.mse_loss(outputs_1_8, inputs_resized)\n","\n","    print(f\"Compression Ratio 1/8 - NMSE: {loss_1_8.item()} - MSE: {loss_mse_1_8.item()}\")\n"],"metadata":{"id":"pkF3zAkFbA_C"},"execution_count":null,"outputs":[]}]}